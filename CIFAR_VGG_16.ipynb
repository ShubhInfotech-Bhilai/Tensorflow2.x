{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CIFAR-VGG-16.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPkZzJI5g+WYWSF05SrDOuc",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/adarsh415/Tensorflow2.x/blob/master/CIFAR_VGG_16.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9FkrGXbb6dUe",
        "colab_type": "code",
        "outputId": "b26758f7-9800-414e-d4ae-d0f509d9f1c9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%tensorflow_version 2.x\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import models, metrics, optimizers, layers, datasets, regularizers\n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 2.x selected.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u-TmTgxr64Ua",
        "colab_type": "code",
        "outputId": "24026d67-94cf-4924-8172-1c12cee488af",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "tf.__version__"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'2.1.0'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jaVgVR846uNr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_data():\n",
        "    (xtrain, ytrain), (xtest, ytest) = datasets.cifar10.load_data()\n",
        "    print('train shape:', xtrain.shape, xtest.shape)\n",
        "    print('test shape:', xtest.shape, ytest.shape)\n",
        "    xtrain = xtrain.astype(np.float32)/255.0\n",
        "    xtest = xtest.astype(np.float32)/255.0\n",
        "    trainDS = tf.data.Dataset.from_tensor_slices((xtrain, ytrain))\n",
        "    trainDS = trainDS.shuffle(50000).batch(64)\n",
        "    testDS = tf.data.Dataset.from_tensor_slices((xtest, ytest))\n",
        "    testDS = testDS.batch(128)\n",
        "    \n",
        "    return trainDS, testDS"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uW6IOUnH60TE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "config = {\n",
        "    \"filters\" : [64,128,256,512],\n",
        "    \"conv\" : 3,\n",
        "    \"num_classes\": 10,\n",
        "    \"input_shape\": (32,32,3)\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SLhkTzD7624C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class VGG16(models.Model):\n",
        "    \n",
        "    def __init__(self, **kwargs):\n",
        "        \n",
        "        super(VGG16, self).__init__(name='VGG16')\n",
        "        \n",
        "        filters = kwargs['filters']\n",
        "        self.layer1_f = filters[0]\n",
        "        self.layer2_f = filters[1]\n",
        "        self.layer3_f = filters[2]\n",
        "        self.layer4_f = filters[3]\n",
        "        self.conv = kwargs['conv']\n",
        "        self.num_classes = kwargs['num_classes']\n",
        "        self.input_shapes = kwargs['input_shape']\n",
        "        #print(filters)\n",
        "        self.conv0 = layers.Conv2D(filters=self.layer1_f, kernel_size=self.conv, input_shape=self.input_shapes, \n",
        "                                   activation='relu', padding='same', kernel_regularizer=regularizers.l2(0.0001))\n",
        "        self.conv1 = layers.Conv2D(filters=self.layer1_f, kernel_size=(self.conv,self.conv), padding='same',activation='relu',\n",
        "                                   kernel_regularizer=regularizers.l2(0.0001))\n",
        "        self.maxp1 = layers.MaxPool2D(pool_size=(2,2))\n",
        "        \n",
        "        self.conv2 = layers.Conv2D(filters=self.layer2_f, kernel_size=self.conv, padding='same', activation='relu',\n",
        "                                   kernel_regularizer=regularizers.l2(0.0001))\n",
        "        self.maxp2 = layers.MaxPool2D(pool_size=(2,2))\n",
        "        \n",
        "        self.conv3 = layers.Conv2D(filters=self.layer3_f, kernel_size=self.conv, padding='same', activation='relu',\n",
        "                                   kernel_regularizer=regularizers.l2(0.0001))\n",
        "        self.maxp3 = layers.MaxPool2D(pool_size=(2,2))\n",
        "        \n",
        "        self.conv4 = layers.Conv2D(filters=self.layer4_f, kernel_size=self.conv,padding='same', activation='relu',\n",
        "                                   kernel_regularizer=regularizers.l2(0.0001))\n",
        "        self.maxp4 = layers.MaxPool2D(pool_size=(2,2))\n",
        "        \n",
        "        self.conv5 = layers.Conv2D(filters=self.layer4_f, kernel_size=self.conv, padding='same',activation='relu',\n",
        "                                   kernel_regularizer=regularizers.l2(0.0001))\n",
        "        self.maxp5 = layers.MaxPool2D(pool_size=(2,2))\n",
        "        \n",
        "        self.fc1 = layers.Dense(512)\n",
        "        self.fc2 = layers.Dense(512)\n",
        "        \n",
        "        self.out = layers.Dense(self.num_classes, activation='softmax')\n",
        "        \n",
        "    def call(self, inputs, training=None):\n",
        "        # first layer\n",
        "        x = self.conv0(inputs)\n",
        "        x = self.conv1(x)\n",
        "        x = self.maxp1(x)\n",
        "        #print('first layer',x.shape)\n",
        "        # second layer\n",
        "        x = self.conv2(x)\n",
        "        x = layers.Conv2D(filters=self.layer2_f, kernel_size=self.conv, padding='same', activation='relu', kernel_regularizer=regularizers.l2(0.0001))(x)\n",
        "        x = layers.BatchNormalization()(x)\n",
        "        x = self.maxp2(x)\n",
        "        x = layers.Dropout(0.4)(x)\n",
        "        \n",
        "        # third layer\n",
        "        x = self.conv3(x)\n",
        "        x = layers.Conv2D(filters=self.layer3_f, kernel_size=self.conv, padding='same', activation='relu', kernel_regularizer=regularizers.l2(0.0001))(x)\n",
        "        x = layers.Conv2D(filters=self.layer3_f, kernel_size=self.conv, padding='same', activation='relu', kernel_regularizer=regularizers.l2(0.0001))(x)\n",
        "        x = layers.BatchNormalization()(x)\n",
        "        x = self.maxp3(x)\n",
        "        x = layers.Dropout(0.3)(x)\n",
        "        \n",
        "        # fourth layer\n",
        "        x = self.conv4(x)\n",
        "        x = layers.Conv2D(filters=self.layer4_f, kernel_size=self.conv,padding='same', activation='relu', kernel_regularizer=regularizers.l2(0.0001))(x)\n",
        "        x = layers.Conv2D(filters=self.layer4_f, kernel_size=self.conv,padding='same', activation='relu', kernel_regularizer=regularizers.l2(0.0001))(x)\n",
        "        x = layers.BatchNormalization()(x)\n",
        "        x = self.maxp4(x)\n",
        "        x = layers.Dropout(0.2)(x)\n",
        "        \n",
        "        # fifth layer\n",
        "        x = self.conv5(x)\n",
        "        x = layers.Conv2D(filters=self.layer4_f, kernel_size=self.conv, padding='same',activation='relu', kernel_regularizer=regularizers.l2(0.0001))(x)\n",
        "        x = layers.Conv2D(filters=self.layer4_f, kernel_size=self.conv, padding='same',activation='relu', kernel_regularizer=regularizers.l2(0.0001))(x)\n",
        "        x = layers.BatchNormalization()(x)\n",
        "        x = self.maxp5(x)\n",
        "        x = layers.Dropout(0.3)(x)\n",
        "        \n",
        "        x = layers.Flatten()(x)\n",
        "        #Fully connected layer\n",
        "        x = self.fc1(x)\n",
        "        x = self.fc2(x)\n",
        "        x = layers.Dropout(0.4)(x)\n",
        "        \n",
        "        #output layer\n",
        "        x = self.out(x)\n",
        "        return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FJQmex4Z7RnK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def compute_loss(logits, labels):\n",
        "    return tf.reduce_mean(tf.losses.categorical_crossentropy(labels, logits))\n",
        "\n",
        "def compute_accuracy(logits, labels):\n",
        "    predictions = tf.argmax(logits, axis=1)\n",
        "    #print('predictions', predictions)\n",
        "    #print('label', labels)\n",
        "    return tf.reduce_mean(tf.cast(tf.equal(predictions, labels ), tf.float32))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ICv6mIPi7VD4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train():\n",
        "    n_epoch = 20\n",
        "    train_ds, test_ds = load_data() \n",
        "    optimizer = optimizers.Adam(learning_rate=0.0001)\n",
        "    model = VGG16(**config)\n",
        "    acc = metrics.CategoricalAccuracy()\n",
        "    lossfn = tf.keras.losses.CategoricalCrossentropy()\n",
        "    for epoch in range(n_epoch):\n",
        "        losses = 0.0\n",
        "        accuracy = 0.0\n",
        "        #step = 0\n",
        "        for step, (x,y) in enumerate(train_ds):\n",
        "            #print(x.shape)\n",
        "            #step += 1\n",
        "            with tf.GradientTape() as tape:\n",
        "                logits = model(x)\n",
        "                #y = tf.squeeze(y, axis=1)\n",
        "                #loss = compute_loss(logits, y)\n",
        "                loss = lossfn(tf.one_hot(y,10),logits)\n",
        "                acc.update_state(tf.one_hot(y,10), logits)\n",
        "                losses = loss\n",
        "            grads = tape.gradient(loss, model.trainable_variables)\n",
        "            grads = [tf.clip_by_norm(g, 15) for g in grads]\n",
        "            optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "            \n",
        "            if step%1000 == 0:\n",
        "                #accuracy = compute_accuracy(logits, tf.cast(y, tf.int64))\n",
        "                #print(f' accuracy in epoch {epoch} after steps {step} is: {accuracy.numpy()}')\n",
        "                print(f' accuracy in epoch {epoch} after steps {step} is: {acc.result().numpy()}')\n",
        "                acc.reset_states()\n",
        "                #print(f' Loss in epoch {epoch} after steps {step} is: {losses}')\n",
        "                print(f' Loss in epoch {epoch} after steps {step} is: {losses.numpy()}')\n",
        "        print(f'loss after epoch {epoch} is {losses.numpy()}')\n",
        "    print(f'final training accuracy is {acc.result().numpy()}')\n",
        "    acc.reset_states()  \n",
        "    #model.save('VGG16', save_format='tf')\n",
        "    for step, (x, y) in enumerate(test_ds):\n",
        "        logits = model(x, training=False)\n",
        "        acc.update_state(tf.one_hot(y,10), logits)\n",
        "        # if step%1000 == 0:\n",
        "        #     #y = tf.squeeze(y, axis=1)\n",
        "        #     #accuracy = compute_accuracy(logits,tf.cast(y, tf.int64))\n",
        "            \n",
        "    print(f'test accuracy is {acc.result().numpy()}')\n",
        "    acc.reset_states()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6MrttK2_7YOu",
        "colab_type": "code",
        "outputId": "e44d4fe9-5fd5-4074-b364-94759a1a0076",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "train()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "170500096/170498071 [==============================] - 6s 0us/step\n",
            "train shape: (50000, 32, 32, 3) (10000, 32, 32, 3)\n",
            "test shape: (10000, 32, 32, 3) (10000, 1)\n",
            " accuracy in epoch 0 after steps 0 is: 0.06689453125\n",
            " Loss in epoch 0 after steps 0 is: 2.3025686740875244\n",
            "loss after epoch 0 is 2.299431324005127\n",
            " accuracy in epoch 1 after steps 0 is: 0.09995273500680923\n",
            " Loss in epoch 1 after steps 0 is: 2.301731586456299\n",
            "loss after epoch 1 is 2.3019790649414062\n",
            " accuracy in epoch 2 after steps 0 is: 0.09965391457080841\n",
            " Loss in epoch 2 after steps 0 is: 2.303251028060913\n",
            "loss after epoch 2 is 2.304696798324585\n",
            " accuracy in epoch 3 after steps 0 is: 0.0990837812423706\n",
            " Loss in epoch 3 after steps 0 is: 2.3022027015686035\n",
            "loss after epoch 3 is 2.2991504669189453\n",
            " accuracy in epoch 4 after steps 0 is: 0.1020594909787178\n",
            " Loss in epoch 4 after steps 0 is: 2.301455497741699\n",
            "loss after epoch 4 is 2.302205801010132\n",
            " accuracy in epoch 5 after steps 0 is: 0.09839361160993576\n",
            " Loss in epoch 5 after steps 0 is: 2.3030970096588135\n",
            "loss after epoch 5 is 2.304574728012085\n",
            " accuracy in epoch 6 after steps 0 is: 0.09868868440389633\n",
            " Loss in epoch 6 after steps 0 is: 2.304288148880005\n",
            "loss after epoch 6 is 2.302349805831909\n",
            " accuracy in epoch 7 after steps 0 is: 0.09912879019975662\n",
            " Loss in epoch 7 after steps 0 is: 2.302731513977051\n",
            "loss after epoch 7 is 2.304790496826172\n",
            " accuracy in epoch 8 after steps 0 is: 0.09844862669706345\n",
            " Loss in epoch 8 after steps 0 is: 2.299527168273926\n",
            "loss after epoch 8 is 2.3006162643432617\n",
            " accuracy in epoch 9 after steps 0 is: 0.098868727684021\n",
            " Loss in epoch 9 after steps 0 is: 2.3033978939056396\n",
            "loss after epoch 9 is 2.3025355339050293\n",
            " accuracy in epoch 10 after steps 0 is: 0.10032407939434052\n",
            " Loss in epoch 10 after steps 0 is: 2.299281597137451\n",
            "loss after epoch 10 is 2.297511577606201\n",
            " accuracy in epoch 11 after steps 0 is: 0.09816855937242508\n",
            " Loss in epoch 11 after steps 0 is: 2.3023264408111572\n",
            "loss after epoch 11 is 2.3043603897094727\n",
            " accuracy in epoch 12 after steps 0 is: 0.09792350232601166\n",
            " Loss in epoch 12 after steps 0 is: 2.3019521236419678\n",
            "loss after epoch 12 is 2.304624557495117\n",
            " accuracy in epoch 13 after steps 0 is: 0.10000400245189667\n",
            " Loss in epoch 13 after steps 0 is: 2.302508592605591\n",
            "loss after epoch 13 is 2.3019118309020996\n",
            " accuracy in epoch 14 after steps 0 is: 0.09855365008115768\n",
            " Loss in epoch 14 after steps 0 is: 2.3025333881378174\n",
            "loss after epoch 14 is 2.3030645847320557\n",
            " accuracy in epoch 15 after steps 0 is: 0.09919380396604538\n",
            " Loss in epoch 15 after steps 0 is: 2.302537202835083\n",
            "loss after epoch 15 is 2.3032195568084717\n",
            " accuracy in epoch 16 after steps 0 is: 0.09828358888626099\n",
            " Loss in epoch 16 after steps 0 is: 2.302417039871216\n",
            "loss after epoch 16 is 2.3020095825195312\n",
            " accuracy in epoch 17 after steps 0 is: 0.09940385818481445\n",
            " Loss in epoch 17 after steps 0 is: 2.302037000656128\n",
            "loss after epoch 17 is 2.3025269508361816\n",
            " accuracy in epoch 18 after steps 0 is: 0.09834985435009003\n",
            " Loss in epoch 18 after steps 0 is: 2.3029298782348633\n",
            "loss after epoch 18 is 2.303704023361206\n",
            " accuracy in epoch 19 after steps 0 is: 0.09786848723888397\n",
            " Loss in epoch 19 after steps 0 is: 2.3031156063079834\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0-cmlbdo7kQM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}